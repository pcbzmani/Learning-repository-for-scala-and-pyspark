{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "date_range.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiJ4+z251f/4l2G4bYkkjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcbzmani/Learning-repository-for-scala-and-pyspark/blob/main/Solutions/Python/date_range.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLqQ2Foo93EZ",
        "outputId": "633b0717-7d9c-49e6-a387-62f3e4e1b26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 41 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 63.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=f293192d45f0c54135c5566e75dd0ca8dcdbc4f6b63493d8cddc7fe7c8064854\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"daterange\")\\\n",
        "        .config('spark.ui.port', '4040')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "aIjyyKwV-8yT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_rnge = [('2021-01-01','2021-01-31'),('2021-02-01','2021-02-28'),('2021-03-01','2021-03-26')]\n",
        "cols_dtr = ['start_dt','end_dt']"
      ],
      "metadata": {
        "id": "xLrAaCQc9-YA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(data_rnge)\n",
        "date_rng_df = rdd.toDF(cols_dtr)"
      ],
      "metadata": {
        "id": "M_nnrq1r-bL5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_rng_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W81CweUO_Dn9",
        "outputId": "17a8fe90-8ef0-4c8d-a19b-a4e76f22d135"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|  start_dt|    end_dt|\n",
            "+----------+----------+\n",
            "|2021-01-01|2021-01-31|\n",
            "|2021-02-01|2021-02-28|\n",
            "|2021-03-01|2021-03-26|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "each_day=[('2021-01-01',30),('2021-01-02',45),('2021-01-29',100),('2021-02-01',890),('2021-02-22',895),('2021-03-01',100)]\n",
        "cols_each_day = ['date_sold','amount_sold']"
      ],
      "metadata": {
        "id": "NHHXjAHn_GUs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize(each_day)\n",
        "date_sold_df = rdd1.toDF(cols_each_day)"
      ],
      "metadata": {
        "id": "hnfRFn8D_jMh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_sold_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd_0j4g-_oqp",
        "outputId": "3332e5d1-6aa2-4162-afe9-2e5e82772e2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "| date_sold|amount_sold|\n",
            "+----------+-----------+\n",
            "|2021-01-01|         30|\n",
            "|2021-01-02|         45|\n",
            "|2021-01-29|        100|\n",
            "|2021-02-01|        890|\n",
            "|2021-02-22|        895|\n",
            "|2021-03-01|        100|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join_cond = [date_sold_df.date_sold >=date_rng_df.start_dt,date_sold_df.date_sold <= date_rng_df.end_dt]\n",
        "join_type = 'inner'"
      ],
      "metadata": {
        "id": "WbmByVmV_xv2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = date_sold_df.join(date_rng_df,join_cond,join_type)"
      ],
      "metadata": {
        "id": "yokSCAnvAKAB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq8Q6ZBlAZNj",
        "outputId": "a380e843-a96b-42e1-a7fd-ac53d743d835"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+----------+----------+\n",
            "| date_sold|amount_sold|  start_dt|    end_dt|\n",
            "+----------+-----------+----------+----------+\n",
            "|2021-01-01|         30|2021-01-01|2021-01-31|\n",
            "|2021-01-02|         45|2021-01-01|2021-01-31|\n",
            "|2021-01-29|        100|2021-01-01|2021-01-31|\n",
            "|2021-02-01|        890|2021-02-01|2021-02-28|\n",
            "|2021-02-22|        895|2021-02-01|2021-02-28|\n",
            "|2021-03-01|        100|2021-03-01|2021-03-26|\n",
            "+----------+-----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum"
      ],
      "metadata": {
        "id": "Q7naQ2OLAsuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = joined_df.groupBy('start_dt','end_dt')\\\n",
        ".sum('amount_sold').alias('total_amount')"
      ],
      "metadata": {
        "id": "hXgEDQnoAbZa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S_sp4QLA1_P",
        "outputId": "a01aae18-3443-4744-d705-671a9035f183"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------------+\n",
            "|  start_dt|    end_dt|sum(amount_sold)|\n",
            "+----------+----------+----------------+\n",
            "|2021-03-01|2021-03-26|             100|\n",
            "|2021-01-01|2021-01-31|             175|\n",
            "|2021-02-01|2021-02-28|            1785|\n",
            "+----------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}